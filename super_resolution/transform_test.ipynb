{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "d2l.set_figsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HR_img_path = os.path.join('..', 'learn_gan', 'CycleGan', 'datasets', 'train')\n",
    "LR_img_path = os.path.join('.', 'LR')\n",
    "\n",
    "img = d2l.Image.open(HR_img_path + os.path.join('\\\\2013-11-08 16_45_24.jpg'))\n",
    "d2l.plt.imshow(img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_img = transforms.Compose([\n",
    "    transforms.Resize(128 // 2)\n",
    "])(img)\n",
    "d2l.plt.imshow(lr_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hr_img = transforms.Compose([\n",
    "    transforms.CenterCrop(128)\n",
    "])(img)\n",
    "d2l.plt.imshow(hr_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "origin_img_list = glob.glob('../learn_gan/CycleGan/datasets/train/*.*')\n",
    "origin_img_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for img_path in origin_img_list:\n",
    "    hr_img = Image.open(img_path)\n",
    "    lr_img = transforms.Compose([transforms.Resize(128 // 2)])(hr_img)\n",
    "    hr_img.save('HR/' + str(idx) + '.jpg')\n",
    "    lr_img.save('LR/' + str(idx) + '.jpg')\n",
    "    idx += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImgLoader(Dataset):\n",
    "    def __init__(self, root_path='../super_resolution'):\n",
    "        self.transformer = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        self.lr_list = sorted(glob.glob(root_path + '/LR/*.*'))\n",
    "        self.hr_list = sorted(glob.glob(root_path + '/HR/*.*'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.lr_list), len(self.hr_list))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'lr_img_path': self.lr_list[index],\n",
    "            'hr_img_path': self.hr_list[index]\n",
    "        }\n",
    "\n",
    "\n",
    "loader = DataLoader(ImgLoader(),\n",
    "                    batch_size=4, shuffle=True, drop_last=True)\n",
    "for i, dicts in enumerate(loader):\n",
    "    print(\"===============================\")\n",
    "    print('lr_path = ')\n",
    "    print(dicts['lr_img_path'])\n",
    "    print('hr_path = ')\n",
    "    print(dicts['hr_img_path'])\n",
    "    print(\"===============================\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import model_g\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "img_hr_path = 'E:\\毕设\\style_transfer\\\\res_img\\\\res2023_03_15_14_32_49.jpg'\n",
    "hr_img = Image.open(img_hr_path)\n",
    "hr_img.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "generator = model_g.Generator()\n",
    "param_dict = torch.load('./output/dicts_p.pt')\n",
    "generator.load_state_dict(param_dict['generator'])\n",
    "generator.to(device)\n",
    "generator.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lr_img = transforms.Compose([\n",
    "    transforms.Resize(128)\n",
    "])(hr_img)\n",
    "lr_img.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "img = transform(lr_img).to(device)\n",
    "pre = generator(img[None, ...])[0]\n",
    "\n",
    "def UnNormalize(img, mean=None, std=None):\n",
    "        if mean is None:\n",
    "            mean = [0.485, 0.456, 0.406]\n",
    "        if std is None:\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "        for i in range(3):\n",
    "            img[i] = std[i]*img[i] + mean[i]\n",
    "        return img\n",
    "\n",
    "pre = UnNormalize(pre).cpu().clamp(0.0, 1.0)\n",
    "sr_img = transforms.ToPILImage()(pre)\n",
    "sr_img.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
